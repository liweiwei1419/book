(window.webpackJsonp=window.webpackJsonp||[]).push([[41],{405:function(t,a,s){"use strict";s.r(a);var m=s(26),c=Object(m.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("div",{staticClass:"custom-block tip"},[s("p",{staticClass:"custom-block-title"},[t._v("TIP")]),t._v(" "),s("p",[t._v("本文介绍了 EM 算法。")])]),t._v(" "),s("p",[t._v("EM 算法，即期望最大化（Expectation-Maximum）算法，用于含有隐变量的概率模型参数的极大似然估计。区别于微积分中通过求导得到最优解的方法，EM 算法是一种迭代算法，并不保证能够得到全局最优解，但可以得到一个局部最优解。")]),t._v(" "),s("p",[t._v("我们所熟知的 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"k"}})],1)],1)],1),t._v(" 均值聚类算法其实是 EM 算法的一个特例。")],1),t._v(" "),s("h3",{attrs:{id:"em-算法的基本思想"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#em-算法的基本思想"}},[t._v("#")]),t._v(" EM 算法的基本思想")]),t._v(" "),s("p",[t._v("EM 算法的思想是"),s("strong",[t._v("先固定其中一个，推测另一个，如此反复")]),t._v("。")]),t._v(" "),s("p",[t._v("例如：模型中有两个未知参数 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"A"}})],1)],1)],1),t._v(" 和 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"B"}})],1)],1)],1),t._v(" 需要估计，而 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"A"}})],1)],1)],1),t._v(" 和 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"B"}})],1)],1)],1),t._v(" 又存在相互依赖的关系，即知道 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"A"}})],1)],1)],1),t._v(" 才能推出 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"B"}})],1)],1)],1),t._v("，知道了 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"B"}})],1)],1)],1),t._v(" 才能推出 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"A"}})],1)],1)],1),t._v(" ，这样的问题就可以用 EM 算法。")],1),t._v(" "),s("p",[t._v("使用极大似然估计，通过迭代算法，既能估计模型的参数，并且还能得到隐变量的值。注意：EM 算法"),s("strong",[t._v("并不保证得到全局最优解，但是可以得到局部最优解，这一点和梯度下降法是一样的，最终的解和初值有关")]),t._v("，这一点在 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"k"}})],1)],1)],1),t._v(" 均值聚类算法中是一样的。")],1),t._v(" "),s("h3",{attrs:{id:"em-算法用于解决含有隐含变量的概率模型的参数估计问题"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#em-算法用于解决含有隐含变量的概率模型的参数估计问题"}},[t._v("#")]),t._v(" EM 算法用于解决含有隐含变量的概率模型的参数估计问题")]),t._v(" "),s("p",[t._v("EM 算法是对概率模型进行参数估计是一种常见的问题分析的方法。当然概率模型仅存在观测数据的时候，可以直接利用最大似然估计的方法，对似然函数取对数，令各个参数的偏导数为 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mn",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"0"}})],1)],1)],1),t._v("，求得的参数的值作为参数的估计。")],1),t._v(" "),s("p",[t._v("但是如果模型含有隐变量，并且隐函数和模型参数是互相影响的，就可以通过使用 EM 算法，通过迭代地求解隐含变量的充分统计量和最大化似然函数以达到参数估计的算法。这样即求得了隐变量，还得到了问题的参数估计。")]),t._v(" "),s("p",[t._v("EM 算法刚接触的时候感觉很晦涩，不过如果熟悉了"),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"k"}})],1)],1)],1),t._v(" 均值聚类算法，往 EM 算法上靠就会发现， "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"k"}})],1)],1)],1),t._v(" 均值聚类算法其实就是在执行 EM 算法这个框架。先学习 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"k"}})],1)],1)],1),t._v(" 均值聚类，再来看 EM 算法，或许入门会简单一些。")],1),t._v(" "),s("h3",{attrs:{id:"通过-均值聚类算法学习-em-算法"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#通过-均值聚类算法学习-em-算法"}},[t._v("#")]),t._v(" 通过 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"k"}})],1)],1)],1),t._v(" 均值聚类算法学习 EM 算法")],1),t._v(" "),s("p",[t._v("这里“同类数据点到其中心的距离之和最短”就等价于似然函数最大。")]),t._v(" "),s("p",[t._v("学习 EM 算法的时候，很容易把自己绕晕，陷入“鸡生蛋、蛋生鸡”的循环，不过可以通过 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"k"}})],1)],1)],1),t._v(" 均值聚类理解 EM 算法的 E 步和 M 步。在 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"k"}})],1)],1)],1),t._v(" 均值聚类中，首先要明确“模型参数”和“隐变量”分别是什么？")],1),t._v(" "),s("p",[t._v("1、每个聚类簇的质心是“隐变量”，“隐变量”决定了一个数据属于哪一个类别，一个数据属于距离它最近的质心所所属的类别。")]),t._v(" "),s("p",[t._v("2、我们要求的是哪些数据可以归为一类，这我们可以理解为是“模型的参数”，是我们直接要求的；")]),t._v(" "),s("p",[t._v("接下来，我们对比  "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"k"}})],1)],1)],1),t._v(" 均值聚类算法和 EM 算法：")],1),t._v(" "),s("table",[s("thead",[s("tr",[s("th",[s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"k"}})],1)],1)],1),t._v(" 均值聚类算法")],1),t._v(" "),s("th",[t._v("EM 算法")]),t._v(" "),s("th",[t._v("说明")])])]),t._v(" "),s("tbody",[s("tr",[s("td"),t._v(" "),s("td",[t._v("（1）选择参数的初值，开始迭代；")]),t._v(" "),s("td")]),t._v(" "),s("tr",[s("td",[t._v("（1）首先选择 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"k"}})],1)],1)],1),t._v(" 个类别的中心；（3）然后更新每个样本的均值，作为类的新的中心。")],1),t._v(" "),s("td",[t._v("（2）E 步：记 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msup",[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3B8"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"i"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1)],1)],1)],1)],1)],1),t._v(" 为第 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"i"}})],1)],1)],1),t._v(" 次迭代参数 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3B8"}})],1)],1)],1),t._v(" 的估计值，在第 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"i"}})],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"3"}},[s("mjx-c",{attrs:{c:"+"}})],1),s("mjx-mn",{staticClass:"mjx-n",attrs:{space:"3"}},[s("mjx-c",{attrs:{c:"1"}})],1)],1)],1),t._v(" 次迭代的 E 步，计算 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"Q"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3B8"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:","}})],1),s("mjx-msup",{attrs:{space:"2"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3B8"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"i"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1)],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1)],1)],1),t._v("；")],1),t._v(" "),s("td",[t._v("求出隐变量")])]),t._v(" "),s("tr",[s("td",[t._v("（2）将样本逐个指派到与其最近的中心的类中，得到一个聚类的结果。")]),t._v(" "),s("td",[t._v("（3）M 步：求使 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"Q"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3B8"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:","}})],1),s("mjx-msup",{attrs:{space:"2"}},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3B8"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"i"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1)],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1)],1)],1),t._v(" 极大化的 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3B8"}})],1)],1)],1),t._v("，确定第 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"i"}})],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"3"}},[s("mjx-c",{attrs:{c:"+"}})],1),s("mjx-mn",{staticClass:"mjx-n",attrs:{space:"3"}},[s("mjx-c",{attrs:{c:"1"}})],1)],1)],1),t._v(" 次迭代的参数的估计值 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-msup",[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3B8"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[s("mjx-TeXAtom",{attrs:{size:"s"}},[s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"i"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"+"}})],1),s("mjx-mn",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"1"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1)],1)],1)],1)],1)],1),t._v("；")],1),t._v(" "),s("td",[t._v("求出模型参数")])]),t._v(" "),s("tr",[s("td",[t._v("重复第（3）步和第（2）步，直到收敛为止。")]),t._v(" "),s("td",[t._v("（4）重复第（2）步和第（3）步，直到收敛。")]),t._v(" "),s("td")]),t._v(" "),s("tr",[s("td",[t._v("李航《统计学习方法》（第 2 版）P264")]),t._v(" "),s("td",[t._v("李航《统计学习方法》（第 2 版）P178")]),t._v(" "),s("td")])])]),t._v(" "),s("p",[t._v("我又画了一个表格，可能这样看会更清楚一些：")]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",[s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"k"}})],1)],1)],1),t._v(" 均值聚类 聚类算法")],1),t._v(" "),s("th",[t._v("EM 算法")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("第 1 步：随机初始化给出质心，这一步可以认为是求出了隐变量。")]),t._v(" "),s("td",[t._v("E 步：固定模型参数，求隐含变量。")])]),t._v(" "),s("tr",[s("td",[t._v("第 2 步：固定质心，把每个数据分配给最近的质心，这一步可以认为是求出模型参数。")]),t._v(" "),s("td",[t._v("M 步：固定隐含变量，求模型参数。")])]),t._v(" "),s("tr",[s("td",[t._v("第 3 步：把是同类数据点取平均，更新质心，这一步可以认为是求出了隐变量。")]),t._v(" "),s("td",[t._v("E 步：固定模型参数，求隐含变量。")])]),t._v(" "),s("tr",[s("td",[t._v("第 4 步：固定质心，把每个数据分配给最近的质心，这一步可以认为是求出模型参数。")]),t._v(" "),s("td",[t._v("M 步：固定隐含变量，求模型参数。")])]),t._v(" "),s("tr",[s("td",[t._v("重复第 3 步、第 4 步，直到质心不再变化或者满足最大迭代次数位置。")]),t._v(" "),s("td",[t._v("重复 E 步和 M 步。")])])])]),t._v(" "),s("p",[t._v("这里再强调一下："),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"k"}})],1)],1)],1),t._v(" 均值聚类算法的隐变量是质心，模型的参数是每个数据点属于哪个分类。再看看 EM 算法的 E 步和 M 步：")],1),t._v(" "),s("ul",[s("li",[t._v("E  步（固定模型参数，求隐变量）")])]),t._v(" "),s("p",[t._v("同类数据点取平均，其实就是在每一个数据点确定的情况下，求隐变量质心的概率分布。具体说来，即我们已知一些数据点的集合 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"S"}})],1)],1)],1),t._v("，我们想求得一个点 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"o"}})],1)],1)],1),t._v("，使得这个点 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"o"}})],1)],1)],1),t._v(" 与所有集合 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"S"}})],1)],1)],1),t._v(" 中的点的距离之和最小，我们很容易知道，这个点 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"o"}})],1)],1)],1),t._v(" 就应该取成集合 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"S"}})],1)],1)],1),t._v(" 中所有的点的各个分类的平均值（用距离之和对每个分量求偏导，并令偏导数为 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mn",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"0"}})],1)],1)],1),t._v("）")],1),t._v(" "),s("p",[t._v("根据参数初始值或上一次迭代的模型参数来计算出隐性变量的后验概率，其实就是隐变量的期望，作为隐藏变量的现估计值。即在当前估计的参数 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3B8"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"t"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1)],1)],1),t._v(" 的情况下给定 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"X"}})],1)],1)],1),t._v("，计算对数似然函数在条件分布 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"Z"}})],1)],1)],1),t._v(" 下的期望值，即")],1),t._v(" "),s("p",[s("span",{staticClass:"katex-display"},[s("span",{staticClass:"katex"},[s("span",{staticClass:"katex-mathml"},[s("math",[s("semantics",[s("mrow",[s("mi",[t._v("Q")]),s("mo",[t._v("(")]),s("mi",[t._v("θ")]),s("mi",{attrs:{mathvariant:"normal"}},[t._v("∣")]),s("mi",[t._v("θ")]),s("mo",[t._v("(")]),s("mi",[t._v("t")]),s("mo",[t._v(")")]),s("mo",[t._v(")")]),s("mo",[t._v("=")]),s("msub",[s("mi",[t._v("E")]),s("mrow",[s("mi",[t._v("Z")]),s("mi",{attrs:{mathvariant:"normal"}},[t._v("∣")]),s("mi",[t._v("X")]),s("mo",{attrs:{separator:"true"}},[t._v(",")]),s("mi",[t._v("θ")]),s("mo",[t._v("(")]),s("mi",[t._v("t")]),s("mo",[t._v(")")])],1)],1),s("mo",[t._v("[")]),s("mi",[t._v("log")]),s("mi",[t._v("L")]),s("mo",[t._v("(")]),s("mi",[t._v("θ")]),s("mo",{attrs:{separator:"true"}},[t._v(";")]),s("mi",[t._v("X")]),s("mo",{attrs:{separator:"true"}},[t._v(",")]),s("mi",[t._v("Z")]),s("mo",[t._v(")")]),s("mo",[t._v("]")])],1),s("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("Q(\\theta|\\theta(t))=E_{Z|X,\\theta(t)}[\\log L(\\theta;X,Z)]\n")])],1)],1)],1),s("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[s("span",{staticClass:"strut",staticStyle:{height:"0.75em"}}),s("span",{staticClass:"strut bottom",staticStyle:{height:"1.1052em","vertical-align":"-0.3551999999999999em"}}),s("span",{staticClass:"base displaystyle textstyle uncramped"},[s("span",{staticClass:"mord mathit"},[t._v("Q")]),s("span",{staticClass:"mopen"},[t._v("(")]),s("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.02778em"}},[t._v("θ")]),s("span",{staticClass:"mord mathrm"},[t._v("∣")]),s("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.02778em"}},[t._v("θ")]),s("span",{staticClass:"mopen"},[t._v("(")]),s("span",{staticClass:"mord mathit"},[t._v("t")]),s("span",{staticClass:"mclose"},[t._v(")")]),s("span",{staticClass:"mclose"},[t._v(")")]),s("span",{staticClass:"mrel"},[t._v("=")]),s("span",{staticClass:"mord"},[s("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.05764em"}},[t._v("E")]),s("span",{staticClass:"vlist"},[s("span",{staticStyle:{top:"0.18019999999999992em","margin-right":"0.05em","margin-left":"-0.05764em"}},[s("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[s("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),s("span",{staticClass:"reset-textstyle scriptstyle cramped"},[s("span",{staticClass:"mord scriptstyle cramped"},[s("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.07153em"}},[t._v("Z")]),s("span",{staticClass:"mord mathrm"},[t._v("∣")]),s("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.07847em"}},[t._v("X")]),s("span",{staticClass:"mpunct"},[t._v(",")]),s("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.02778em"}},[t._v("θ")]),s("span",{staticClass:"mopen"},[t._v("(")]),s("span",{staticClass:"mord mathit"},[t._v("t")]),s("span",{staticClass:"mclose"},[t._v(")")])])])]),s("span",{staticClass:"baseline-fix"},[s("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[s("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])]),s("span",{staticClass:"mopen"},[t._v("[")]),s("span",{staticClass:"mop"},[t._v("lo"),s("span",{staticStyle:{"margin-right":"0.01389em"}},[t._v("g")])]),s("span",{staticClass:"mord mathit"},[t._v("L")]),s("span",{staticClass:"mopen"},[t._v("(")]),s("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.02778em"}},[t._v("θ")]),s("span",{staticClass:"mpunct"},[t._v(";")]),s("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.07847em"}},[t._v("X")]),s("span",{staticClass:"mpunct"},[t._v(",")]),s("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.07153em"}},[t._v("Z")]),s("span",{staticClass:"mclose"},[t._v(")")]),s("span",{staticClass:"mclose"},[t._v("]")])])])])])]),t._v(" "),s("ul",[s("li",[t._v("M 步（固定隐变量，求模型参数）")])]),t._v(" "),s("p",[t._v("固定隐变量的前提下，求经过隐变量改写的似然函数的极大。质心确定的前提下，每个数据点分给最近的质心就能够使同类数据点到其中心的距离之和最短。找出使上式最大化的参数：")]),t._v("\n\\theta(t+1) = {\\rm argmax}_{\\theta}Q(\\theta|\\theta(t))\n\n"),s("p",[t._v("示例代码：")]),t._v(" "),s("div",{staticClass:"language-python line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sklearn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mixture "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" GaussianMixture\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br")])]),s("p",[t._v("参数有：1、高斯混合模型的个数；2、协方差的类型；3、最大迭代次数。")]),t._v(" "),s("h3",{attrs:{id:"理解-em-算法的迭代步骤"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#理解-em-算法的迭代步骤"}},[t._v("#")]),t._v(" 理解 EM 算法的迭代步骤")]),t._v(" "),s("p",[s("img",{attrs:{src:"http://upload-images.jianshu.io/upload_images/414598-92dac0a5f654d2b1.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240",alt:" EM 算法的迭代步骤"}})]),t._v(" "),s("p",[t._v("首先先找到“紫”线的一个下界函数，即“蓝”线，一定要有重合的那一个“点”，即被图中“绿”线确定的那个点。")]),t._v(" "),s("p",[t._v("然后对“蓝”线取极大值，即被图中“红”线确定的那个点。")]),t._v(" "),s("p",[t._v("如此反复，你会看到，只会逐步来到局部最优值点。")]),t._v(" "),s("h3",{attrs:{id:"公式推导"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#公式推导"}},[t._v("#")]),t._v(" 公式推导")]),t._v(" "),s("p",[t._v("手写笔记，我写在这里了："),s("a",{attrs:{href:"https://www.jianshu.com/p/e57838214b2b",target:"_blank",rel:"noopener noreferrer"}},[t._v("EM 算法手写笔记"),s("OutboundLink")],1),t._v("。")]),t._v(" "),s("p",[t._v("在公式推导的过程中，要用到 Jensen 不等式或者 KL 散度，也称相对熵。关于 Jensen 不等式，我写在这里了："),s("a",{attrs:{href:"https://www.liwei.party/2019/02/18/machine-learning/jenson/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Jenson 不等式的笔记"),s("OutboundLink")],1),t._v("。关于 KL 散度，我写在这里了："),s("a",{attrs:{href:"https://www.jianshu.com/p/2ea0406d0793",target:"_blank",rel:"noopener noreferrer"}},[t._v("信息熵、条件熵、联合熵、互信息、相对熵、交叉熵"),s("OutboundLink")],1),t._v("。")]),t._v(" "),s("p",[t._v("细节一：在使用 Jensen 不等式的时候，需要假设隐变量服从某种形式的概率分布，才可以将推导过程的一部分看成是期望的表达形式从而应用 Jensen 不等式。然而这个分布不是随便指定的。我们令 Jensen 不等式取等号的时候，可以计算出这个分布其实就是："),s("strong",[t._v("已知观测数据的隐变量的后验概率分布")]),t._v("。由于求 Q 函数需要先求出隐变量的后验概率的期望，因此，这就可以解释为什么EM算法的“通俗”理解角度的E步骤是求隐变量的期望了。")]),t._v(" "),s("p",[t._v("细节二：Q 函数与"),s("strong",[t._v("完全数据的对数似然函数")]),t._v("的关系。有时候在用 EM 算法解决某个具体问题的时候，会发现 M 步骤极大化的居然是"),s("strong",[t._v("完全数据的对数似然函数")]),t._v("。这是因为，Q 函数虽然是完全数据的对数似然函数的某种期望，但是"),s("strong",[t._v("求这个期望的过程有时其实就是将隐变量的后验概率的期望代入就可以了")]),t._v("。因此，"),s("strong",[t._v("本质上我们其实还是在求 Q 函数的极大")]),t._v("。")]),t._v(" "),s("h3",{attrs:{id:"高斯混合模型"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#高斯混合模型"}},[t._v("#")]),t._v(" 高斯混合模型")]),t._v(" "),s("p",[t._v("模型假设：多个高斯分布的加权平均（线性组合），权重（系数）之和为 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mn",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"1"}})],1)],1)],1),t._v("。")],1),t._v(" "),s("p",[t._v("隐变量 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"z"}})],1)],1)],1),t._v(" ：样本 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"x"}})],1)],1)],1),t._v(" 属于哪一个高斯分布，"),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"x"}})],1)],1)],1),t._v(" 与 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"z"}})],1)],1)],1),t._v(" 一一对应，因为 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"z"}})],1)],1)],1),t._v(" 是离散型随机变量，因此可以有一个概率分布（可以认为属于两个分布，概率有大有小）。")],1),t._v(" "),s("p",[t._v("生成模型：生成过程如下：1、随机选择一个高斯分布；2、从这个高斯分布生成一个数据。")]),t._v(" "),s("p",[t._v("直接使用极大似然估计，不能得到解析解。")]),t._v(" "),s("p",[t._v("下面是高斯混合模型的例子：")]),t._v(" "),s("p",[t._v("输入：观测数据和类别的总数。")]),t._v(" "),s("p",[t._v("输出：观测数据所服从的几个分布函数的参数。")]),t._v(" "),s("blockquote",[s("p",[t._v("例如：输入：7000 份成绩，来自 4 个科目：语文、数学、英语、计算机。\n输出：4 个科目分别服从的分布的参数值，由于各科成绩服从高斯分布，因此输出为每科成绩的分布参数 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"Y"}})],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"="}})],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"{"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"3BC"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"1"}})],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:","}})],1),s("mjx-msub",{attrs:{space:"2"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"3C3"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"1"}})],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:","}})],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"2"}},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"3BC"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"2"}})],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:","}})],1),s("mjx-msub",{attrs:{space:"2"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"3C3"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"2"}})],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:","}})],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"2"}},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"3BC"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"3"}})],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:","}})],1),s("mjx-msub",{attrs:{space:"2"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"3C3"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"3"}})],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"("}})],1),s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"3BC"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"4"}})],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:","}})],1),s("mjx-msub",{attrs:{space:"2"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"3C3"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"4"}})],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:")"}})],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"}"}})],1)],1)],1),t._v("，以及样本服从各个分布的概率 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mi",{staticClass:"mjx-i"},[s("mjx-c",{attrs:{c:"3D5"}})],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"="}})],1),s("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[s("mjx-c",{attrs:{c:"{"}})],1),s("mjx-msub",[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"3D5"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"1"}})],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:","}})],1),s("mjx-msub",{attrs:{space:"2"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"3D5"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"2"}})],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:","}})],1),s("mjx-msub",{attrs:{space:"2"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"3D5"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"3"}})],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:","}})],1),s("mjx-msub",{attrs:{space:"2"}},[s("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[s("mjx-c",{attrs:{c:"3D5"}})],1),s("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[s("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[s("mjx-c",{attrs:{c:"4"}})],1)],1)],1),s("mjx-mo",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"}"}})],1)],1)],1),t._v("。")],1)]),t._v(" "),s("p",[t._v("EM 算法解决这个的思路是使用启发式的迭代方法，既然我们无法直接求出模型分布参数，那么我们可以先猜想隐含参数（EM 算法的 E 步），接着基于观察数据和猜测的隐含参数一起来极大化对数似然，求解我们的模型参数（EM算法的M步)。由于我们之前的隐含参数是猜测的，所以此时得到的模型参数一般还不是我们想要的结果。我们基于当前得到的模型参数，继续猜测隐含参数（EM算法的 E 步），然后继续极大化对数似然，求解我们的模型参数（EM算法的M步)。以此类推，不断的迭代下去，直到模型分布参数基本无变化，算法收敛，找到合适的模型参数。")]),t._v(" "),s("p",[t._v("这个时候有人就想到我们必须从某一点开始，并用迭代的办法去解决这个问题：我们先设定男生身高和女生身高分布的几个参数（初始值），然后根据这些参数去判断每一个样本（人）是男生还是女生，之后根据标注后的样本再反过来重新估计参数。之后再多次重复这个过程，直至稳定。这个算法也就是EM算法。")]),t._v(" "),s("p",[t._v("又如：得到一堆身高数据，但是不知道这些身高数据是男生还是女生。")]),t._v(" "),s("p",[t._v("对于每一个样本或者你抽取到的人，就有两个问题需要估计了，一是这个人是男的还是女的，二是男生和女生对应的身高的正态分布的参数是多少。这两个问题是相互依赖的。")]),t._v(" "),s("p",[t._v("有了每个人的归属，或者说我们已经大概地按上面的方法将这 200 个人分为男生和女生两部分，我们就可以根据之前说的最大似然那样，\n通过这些被大概分为男生的 n 个人来重新估计第一个分布的参数，女生的那个分布同样方法重新估计。这个是 Maximization。")]),t._v(" "),s("p",[t._v("然后，当我们更新了这两个分布的时候，每一个属于这两个分布的概率又变了，那么我们就再需要调整 E 步。如此往复，直到参数基本不再发生变化为止。")]),t._v(" "),s("p",[t._v("具体方法为：隐变量是一个数据是男生身高数据还是女生身高数据。")]),t._v(" "),s("p",[t._v("E 步：先设定男生和女生的身高分布参数（初始值，即模型参数），例如男生的身高分布为 ， 女生的身高分布为  ，当然了，刚开始肯定没那么准；\n然后计算出每个人更可能属于第一个还是第二个正态分布中的（例如，这个人的身高是 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[s("mjx-math",{staticClass:" MJX-TEX"},[s("mjx-mn",{staticClass:"mjx-n"},[s("mjx-c",{attrs:{c:"1"}}),s("mjx-c",{attrs:{c:"8"}}),s("mjx-c",{attrs:{c:"0"}})],1)],1)],1),t._v("cm，那很明显，他极大可能属于男生），这个是属于 Expectation 一步；")],1),t._v(" "),s("p",[t._v("M 步：求问题参数，我们已经大概地按上面的方法将这 200 个人分为男生和女生两部分，我们就可以根据之前说的极大似然估计分别对男生和女生的身高分布参数进行估计（这不变成了极大似然估计了吗？极大即为Maximization）这步称为 Maximization。")]),t._v(" "),s("p",[t._v("然后，当我们更新这两个分布的时候，每一个学生属于女生还是男生的概率又变了，那么我们就再需要调整 E 步；\n如此往复，直到参数基本不再发生变化或满足结束条件为止。")]),t._v(" "),s("p",[t._v("上面的学生属于男生还是女生我们称之为隐含参数，女生和男生的身高分布参数称为模型参数。")]),t._v(" "),s("p",[t._v("EM 算法解决这个的思路是使用启发式的迭代方法，既然我们无法直接求出模型分布参数，那么我们可以先猜想隐含参数（EM 算法的 E 步），接着基于观察数据和猜测的隐含参数一起来极大化对数似然，求解我们的模型参数（EM 算法的 M 步)。由于我们之前的隐含参数是猜测的，所以此时得到的模型参数一般还不是我们想要的结果。我们基于当前得到的模型参数，继续猜测隐含参数（EM 算法的 E 步），然后继续极大化对数似然，求解我们的模型参数（EM 算法的 M 步)。以此类推，不断的迭代下去，直到模型分布参数基本无变化，算法收敛，找到合适的模型参数。")]),t._v(" "),s("p",[t._v("这个时候有人就想到我们必须从某一点开始，并用迭代的办法去解决这个问题：我们先设定男生身高和女生身高分布的几个参数（初始值），然后根据这些参数去判断每一个样本（人）是男生还是女生，之后根据标注后的样本再反过来重新估计参数。之后再多次重复这个过程，直至稳定。这个算法也就是EM算法。")]),t._v(" "),s("p",[t._v("引入了坐标上升法。")]),t._v(" "),s("p",[t._v("应用： EM 算法有很多的应用，最广泛的就是 GMM 混合高斯模型、聚类、HMM 等等。\n具体可以参考 JerryLead 的 cnblog 中 的Machine Learning 专栏。")]),t._v(" "),s("p",[t._v("应用：隐马尔科夫模型， LDA 主题模型。")]),t._v(" "),s("h2",{attrs:{id:"参考资料"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#参考资料"}},[t._v("#")]),t._v(" 参考资料")]),t._v(" "),s("p",[t._v("[1] 李航. 统计学习方法（第 2 版）第 9 章“EM 算法及其推广”. 北京：清华大学出版社，2019.")]),t._v(" "),s("p",[t._v("说明：公式很多，写得比较晦涩，但是比较全面，理解书上的内容和细节要查阅一些资料。")]),t._v(" "),s("p",[t._v("[2] 周志华. 机器学习（第 7 章第 6 节“EM 算法”）. 北京：清华大学出版社，2017.")]),t._v(" "),s("p",[t._v("（本节完）")]),t._v(" "),s("hr"),t._v(" "),s("p",[t._v("以下为草稿，我自己留着备用，读者可以忽略，欢迎大家批评指正。")]),t._v(" "),s("h3",{attrs:{id:"参考资料-2"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#参考资料-2"}},[t._v("#")]),t._v(" 参考资料")]),t._v(" "),s("p",[t._v("1、知乎：EM算法存在的意义是什么？")]),t._v(" "),s("p",[t._v("https://www.zhihu.com/question/40797593/answer/275171156")]),t._v(" "),s("p",[t._v("2、Orange先生：浅谈 EM 算法的两个理解角度\nhttps://blog.csdn.net/xmu_jupiter/article/details/50936177")]),t._v(" "),s("p",[t._v("说明：这篇文章没有抄公式，把重要的思想部分提取出来。")]),t._v(" "),s("p",[t._v("3、人人都懂EM算法：https://zhuanlan.zhihu.com/p/36331115\n说明：看这篇文章终于知道了个大概。")]),t._v(" "),s("p",[t._v("4、刘建平的文章：https://www.cnblogs.com/pinard/p/6912636.html")]),t._v(" "),s("p",[t._v("说明：如果手边没有《统计学习方法》这本书，可以看这篇博客。")]),t._v(" "),s("p",[t._v("5、Lasso回归算法： 坐标轴下降法与最小角回归法小结：\nhttp://www.cnblogs.com/pinard/p/6018889.html")]),t._v(" "),s("p",[t._v("6、混合高斯模型（Mixtures of Gaussians）和EM算法\nhttps://www.cnblogs.com/jerrylead/archive/2011/04/06/2006924.html")]),t._v(" "),s("p",[t._v("7、数据分析师进阶必备6大数学利器\nhttps://mp.weixin.qq.com/s/BH4hsLjv7rLvR8xOLU1iNQ")]),t._v(" "),s("p",[t._v("8、矩阵论：向量范数和矩阵范数\nhttps://blog.csdn.net/pipisorry/article/details/51030563")]),t._v(" "),s("p",[t._v("9、知乎上的问题：怎么通俗易懂地解释EM算法并且举个例子?\nhttps://www.zhihu.com/question/27976634/answer/163164402")]),t._v(" "),s("p",[t._v("10、从最大似然到EM算法浅解\nhttps://blog.csdn.net/zouxy09/article/details/8537620")]),t._v(" "),s("p",[t._v("11、"),s("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/54831287",target:"_blank",rel:"noopener noreferrer"}},[t._v("深入浅出之EM算法(一）"),s("OutboundLink")],1)]),t._v(" "),s("p",[t._v("12、"),s("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/54890367",target:"_blank",rel:"noopener noreferrer"}},[t._v("深入浅出EM算法（2）"),s("OutboundLink")],1)]),t._v(" "),s("p",[t._v("13、极客时间：")]),t._v(" "),s("p",[s("a",{attrs:{href:"https://time.geekbang.org/column/article/81896",target:"_blank",rel:"noopener noreferrer"}},[t._v("28丨EM聚类（上）：如何将一份菜等分给两个人？"),s("OutboundLink")],1)]),t._v(" "),s("p",[s("a",{attrs:{href:"https://time.geekbang.org/column/article/82333",target:"_blank",rel:"noopener noreferrer"}},[t._v("29丨EM聚类（下）：用EM算法对王者荣耀英雄进行划分"),s("OutboundLink")],1)]),t._v(" "),s("p",[t._v("14、"),s("a",{attrs:{href:"https://blog.csdn.net/zouxy09/article/details/8537620",target:"_blank",rel:"noopener noreferrer"}},[t._v("从最大似然到EM算法浅解"),s("OutboundLink")],1),t._v("。")]),t._v(" "),s("p",[t._v("15、K-Means聚类算法原理\nhttp://www.cnblogs.com/pinard/p/6164214.html")]),t._v(" "),s("p",[t._v("16、用scikit-learn学习K-Means聚类\nhttps://www.cnblogs.com/pinard/p/6169370.html")]),t._v(" "),s("p",[t._v("17、http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html")]),t._v(" "),s("p",[t._v("18、“上帝的算法”——EM\nhttps://blog.csdn.net/sb19931201/article/details/53586468?utm_source=blogxgwz1\n这篇文章主要是摘抄。写了很多参考文献。")]),t._v(" "),s("p",[t._v("说明：这篇文章 EM 算法的推导直接从极大似然估计开始，讲到了我们的目标就是求观测数据的极大似然。")])])}),[],!1,null,null,null);a.default=c.exports}}]);