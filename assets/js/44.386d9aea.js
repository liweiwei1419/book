(window.webpackJsonp=window.webpackJsonp||[]).push([[44],{409:function(t,s,a){"use strict";a.r(s);var m=a(26),c=Object(m.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("div",{staticClass:"custom-block tip"},[a("p",{staticClass:"custom-block-title"},[t._v("TIP")]),t._v(" "),a("p",[t._v("本文介绍了逻辑回归模型。")])]),t._v(" "),a("h1",{attrs:{id:"《统计学习方法》第-6-章-逻辑回归-学习笔记"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#《统计学习方法》第-6-章-逻辑回归-学习笔记"}},[t._v("#")]),t._v(" 《统计学习方法》第 6 章“逻辑回归”学习笔记")]),t._v(" "),a("h3",{attrs:{id:"用于分类"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#用于分类"}},[t._v("#")]),t._v(" 用于分类")]),t._v(" "),a("p",[t._v("逻辑回归是一个分类模型，“逻辑”是 Logistic 的音译。之所以叫回归，是因为它"),a("strong",[t._v("实际上预测的是概率")]),t._v("，即拟合概率，只是“临门一脚”，选择概率最大的类作为预测的分类结果，逻辑回归本质上是一个回归问题。")]),t._v(" "),a("p",[t._v("逻辑回归解决分类问题，且只用于二分类，在本章节的最后，我们沿用二分类的思路可以解决多分类问题。")]),t._v(" "),a("p",[t._v("Logistic 回归是一种思想简单，且应用广泛的分类方法。相对于深度学习而言，深度学习等复杂的机器学习算法，对数据的要求很高，朴素简单的 Logistic 往往有时能达到很好的效果。")]),t._v(" "),a("p",[t._v("在线性回归中，我们找到一组参数 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"3B8"}})],1)],1)],1),t._v(" ，使得 $y = \\theta \\cdot x_b $ 去拟合数据点，这里的 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"y"}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[a("mjx-c",{attrs:{c:"2208"}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[a("mjx-c",{attrs:{c:"("}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"2212"}})],1),a("mjx-mi",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"221E"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:","}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"2"}},[a("mjx-c",{attrs:{c:"+"}})],1),a("mjx-mi",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"221E"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:")"}})],1)],1)],1),t._v("。")],1),t._v(" "),a("p",[t._v("我们知道，"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"("}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"2212"}})],1),a("mjx-mi",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"221E"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:","}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"2"}},[a("mjx-c",{attrs:{c:"+"}})],1),a("mjx-mi",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"221E"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:")"}})],1)],1)],1),t._v(" 有两个极端，对于二分类问题，就使用这两个极端来做分类，越极端越肯定，即越趋于无穷，概率值越趋近于 0 或 1，这里 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"y"}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[a("mjx-c",{attrs:{c:"="}})],1),a("mjx-mn",{staticClass:"mjx-n",attrs:{space:"4"}},[a("mjx-c",{attrs:{c:"0"}})],1)],1)],1),t._v(" 就是这两类的分类边界。下面看一个例子。")],1),t._v(" "),a("p",[t._v("对于一个二分类 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-TeXAtom",[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"A"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:","}})],1),a("mjx-mi",{staticClass:"mjx-i",attrs:{space:"2"}},[a("mjx-c",{attrs:{c:"B"}})],1)],1)],1)],1),t._v(" 问题，如果我们通过逻辑回归计算出来的参数 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"3B8"}})],1)],1)],1),t._v("，使得 $y = \\theta \\cdot x_b $ 的值大于 0，则判定属于 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"A"}})],1)],1)],1),t._v(" 类，如果计算出来 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"y"}})],1)],1)],1),t._v(" 的值小于 0，则判定属于 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"B"}})],1)],1)],1),t._v(" 类。如果 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"y"}})],1)],1)],1),t._v(" 的值越大，则我们越肯定该预测点属于 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"A"}})],1)],1)],1),t._v(" 类，如果 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"y"}})],1)],1)],1),t._v(" 的值越小，则我们越肯定该预测点属于 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"B"}})],1)],1)],1),t._v(" 类。")],1),t._v(" "),a("p",[a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"3B8"}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"3"}},[a("mjx-c",{attrs:{c:"22C5"}})],1),a("mjx-msub",{attrs:{space:"3"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"x"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"b"}})],1)],1)],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[a("mjx-c",{attrs:{c:"="}})],1),a("mjx-mn",{staticClass:"mjx-n",attrs:{space:"4"}},[a("mjx-c",{attrs:{c:"0"}})],1)],1)],1),t._v(" 是一个超平面，在三维空间中，就是一个平面，在二维空间中，就是一条直线，根据点到超平面的距离公式：\n"),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mfrac",[a("mjx-frac",[a("mjx-num",[a("mjx-nstrut"),a("mjx-mrow",{attrs:{size:"s"}},[a("mjx-mpadded",[a("mjx-block",{staticStyle:{margin:"0.896em 0 0.313em"}},[a("mjx-mrow")],1)],1),a("mjx-mstyle",{staticStyle:{"font-size":"141.4%"}},[a("mjx-TeXAtom",[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"3B8"}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"3"}},[a("mjx-c",{attrs:{c:"22C5"}})],1),a("mjx-msub",{attrs:{space:"3"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"x"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"b"}})],1)],1)],1)],1)],1)],1)],1),a("mjx-dbox",[a("mjx-dtable",[a("mjx-line"),a("mjx-row",[a("mjx-den",[a("mjx-dstrut"),a("mjx-mrow",{attrs:{size:"s"}},[a("mjx-mpadded",[a("mjx-block",{staticStyle:{margin:"0.896em 0 0.313em"}},[a("mjx-mrow")],1)],1),a("mjx-mstyle",{staticStyle:{"font-size":"141.4%"}},[a("mjx-TeXAtom",[a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"|"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"|"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"3B8"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"|"}})],1),a("mjx-msup",[a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"|"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[a("mjx-mn",{staticClass:"mjx-n",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"2"}})],1)],1)],1)],1)],1)],1)],1)],1)],1)],1)],1)],1)],1)],1),t._v("，$\\theta \\cdot x_b $ 的绝对值越大，则表明预测点离这条直线越远，则我们越敢肯定，预测点属于其中一个类别。")],1),t._v(" "),a("ul",[a("li",[t._v("$\\theta \\cdot x_b $ 的符号，决定了预测点的类别，在逻辑回归中，我们定义类别为“1”和“0”。\n说明：我们完全可以将类别定义为“A”、“B”，或者 1、-1（在 SVM 算法中，就是这么定义的）。我们的定义总是为了我们后续优化计算方便，这一点可以在以后的学习中逐渐体会。")]),t._v(" "),a("li",[a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"|"}})],1),a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"3B8"}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"3"}},[a("mjx-c",{attrs:{c:"22C5"}})],1),a("mjx-msub",{attrs:{space:"3"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"x"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"b"}})],1)],1)],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"|"}})],1)],1)],1),t._v(" 的大小，决定了我们对预测点属于某一个类别的肯定程度。例如我们从小到大的百分制考试中，以 60 为分界线，离 60 越远的分数，例如 10 分或者 90 分，我们总是能肯定对这一部分知识的掌握越好或者越坏，可以选拔优秀或者淘汰不合格，越靠近 60 的分数，其实可上可下，我们不好做优劣判断，这与逻辑回归是一个道理。")],1)]),t._v(" "),a("p",[t._v("将上面的思路，写成数学表达式就是，我们期望找到一个函数，将 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"y"}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[a("mjx-c",{attrs:{c:"="}})],1),a("mjx-mi",{staticClass:"mjx-i",attrs:{space:"4"}},[a("mjx-c",{attrs:{c:"3B8"}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"3"}},[a("mjx-c",{attrs:{c:"22C5"}})],1),a("mjx-msub",{attrs:{space:"3"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{noIC:"true"}},[a("mjx-c",{attrs:{c:"x"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"-0.15em"}},[a("mjx-mi",{staticClass:"mjx-i",attrs:{size:"s"}},[a("mjx-c",{attrs:{c:"b"}})],1)],1)],1)],1)],1),t._v(" 送入，得到一个概率，这个概率越趋近于 1 ，则表示判定属于其中一类的概率越大，这个概率越趋近于 0 ，则表示不属于这一类（即属于另一类）的概率越大。区分类别的概率边界值就是 0.5。")],1),t._v(" "),a("p",[t._v("前辈们已经为我们找到了这个函数，命名为 sigmoid 函数，有的地方又称之为逻辑函数，它的数学表达式如下：")]),t._v("\n{\\rm sigmoid}(t) = \\frac{1}{1 + e^{-t}}\n\n"),a("p",[t._v("于是，我们得到的逻辑回归模型就是这样一个模型：")]),t._v("\n{\\rm sigmoid}(x) = \\cfrac{1}{1 + e^{-\\theta \\cdot x}}\n\n"),a("p",[t._v("以下对损失函数的推导中，我们总是定义，当 ${\\rm sigmoid}(x) >= 0.5 $ 时，类别 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"y"}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[a("mjx-c",{attrs:{c:"="}})],1),a("mjx-mn",{staticClass:"mjx-n",attrs:{space:"4"}},[a("mjx-c",{attrs:{c:"1"}})],1)],1)],1),t._v("，当 ${\\rm sigmoid}(x) < 0.5 $，类别 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"y"}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"4"}},[a("mjx-c",{attrs:{c:"="}})],1),a("mjx-mn",{staticClass:"mjx-n",attrs:{space:"4"}},[a("mjx-c",{attrs:{c:"0"}})],1)],1)],1),t._v("。")],1),t._v(" "),a("h3",{attrs:{id:"广义线性回归"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#广义线性回归"}},[t._v("#")]),t._v(" 广义线性回归")]),t._v(" "),a("p",[t._v("逻辑回归是广义的线性回归，预测房价是回归问题，但是否购买房子就是一个分类问题。因此逻辑回归可以看成“线性部分 + 非线性处理”，而“线性部分 + 非线性处理”也是深度学习的神经网络中神经元的表达形式。")]),t._v(" "),a("p",[t._v("我是这样理解广义线性回归的：")]),t._v(" "),a("p",[t._v("Sigmoid 函数可以用于概率计算：")]),t._v("\ny = \\cfrac{1}{1+e^{-\\vec w \\cdot \\vec x}}\n\n"),a("p",[t._v("将 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-msup",[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"e"}})],1),a("mjx-script",{staticStyle:{"vertical-align":"0.363em"}},[a("mjx-TeXAtom",{attrs:{size:"s"}},[a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"2212"}})],1),a("mjx-TeXAtom",[a("mjx-mover",[a("mjx-over",{staticStyle:{"padding-bottom":"0.06em","padding-left":"0.191em","margin-bottom":"-0.516em"}},[a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"20D7"}})],1)],1),a("mjx-base",[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"w"}})],1)],1)],1)],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"22C5"}})],1),a("mjx-TeXAtom",[a("mjx-mover",[a("mjx-over",{staticStyle:{"padding-bottom":"0.06em","padding-left":"0.064em","margin-bottom":"-0.516em"}},[a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"20D7"}})],1)],1),a("mjx-base",[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"x"}})],1)],1)],1)],1)],1)],1)],1)],1)],1),t._v(" 写成 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"y"}})],1)],1)],1),t._v(" 的函数，即")],1),t._v("\ne^{-\\vec w \\cdot \\vec x} = \\cfrac{1-y}{y}\n\n"),a("p",[t._v("两边同时取对数，再乘以 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"2212"}})],1),a("mjx-mn",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"1"}})],1)],1)],1),t._v("，得")],1),t._v("\n\\vec w \\cdot \\vec x = \\ln \\cfrac{y}{1-y}\n\n"),a("p",[t._v("因此，对概率 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"y"}})],1)],1)],1),t._v(" 做一个非线性变换 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mi",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"l"}}),a("mjx-c",{attrs:{c:"n"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"2061"}})],1),a("mjx-mfrac",{attrs:{space:"2"}},[a("mjx-frac",[a("mjx-num",[a("mjx-nstrut"),a("mjx-mrow",{attrs:{size:"s"}},[a("mjx-mpadded",[a("mjx-block",{staticStyle:{margin:"0.896em 0 0.313em"}},[a("mjx-mrow")],1)],1),a("mjx-mstyle",{staticStyle:{"font-size":"141.4%"}},[a("mjx-TeXAtom",[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"y"}})],1)],1)],1)],1)],1),a("mjx-dbox",[a("mjx-dtable",[a("mjx-line"),a("mjx-row",[a("mjx-den",[a("mjx-dstrut"),a("mjx-mrow",{attrs:{size:"s"}},[a("mjx-mpadded",[a("mjx-block",{staticStyle:{margin:"0.896em 0 0.313em"}},[a("mjx-mrow")],1)],1),a("mjx-mstyle",{staticStyle:{"font-size":"141.4%"}},[a("mjx-TeXAtom",[a("mjx-mn",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"1"}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"3"}},[a("mjx-c",{attrs:{c:"2212"}})],1),a("mjx-mi",{staticClass:"mjx-i",attrs:{space:"3"}},[a("mjx-c",{attrs:{c:"y"}})],1)],1)],1)],1)],1)],1)],1)],1)],1)],1)],1)],1),t._v(" ，它就是自变量 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-TeXAtom",[a("mjx-mover",[a("mjx-over",{staticStyle:{"padding-bottom":"0.06em","padding-left":"0.064em","margin-bottom":"-0.516em"}},[a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"20D7"}})],1)],1),a("mjx-base",[a("mjx-mi",{staticClass:"mjx-i"},[a("mjx-c",{attrs:{c:"x"}})],1)],1)],1)],1)],1)],1),t._v(" 的线性函数，因此是一个回归问题。")],1),t._v(" "),a("h3",{attrs:{id:"逻辑回归的损失函数"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#逻辑回归的损失函数"}},[t._v("#")]),t._v(" 逻辑回归的损失函数")]),t._v(" "),a("p",[t._v("损失函数具有这样的性质：它度量了预测值和真实值的差别，对于一个样本而言它是非负数，且当预测值与真实值越接近时，损失函数的值越小，当预测值与真实值差别越大时，损失函数的值应该越大。")]),t._v(" "),a("p",[t._v("我们需要一个函数，这个函数可以度量概率和损失的关系，负对数函数就是这样一个函数，把概率从 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"["}})],1),a("mjx-mn",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"0"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:","}})],1),a("mjx-mn",{staticClass:"mjx-n",attrs:{space:"2"}},[a("mjx-c",{attrs:{c:"1"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"]"}})],1)],1)],1),t._v(" 映射到损失 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"["}})],1),a("mjx-mn",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"0"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:","}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"2"}},[a("mjx-c",{attrs:{c:"+"}})],1),a("mjx-mi",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"221E"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"]"}})],1)],1)],1),t._v("。因为负对数可以理解为先取倒数再取对数，取导数把 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"["}})],1),a("mjx-mn",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"0"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:","}})],1),a("mjx-mn",{staticClass:"mjx-n",attrs:{space:"2"}},[a("mjx-c",{attrs:{c:"1"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"]"}})],1)],1)],1),t._v(" 映射到 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"["}})],1),a("mjx-mn",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"1"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:","}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"2"}},[a("mjx-c",{attrs:{c:"+"}})],1),a("mjx-mi",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"221E"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"]"}})],1)],1)],1),t._v("，取对数把 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"["}})],1),a("mjx-mn",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"1"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:","}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"2"}},[a("mjx-c",{attrs:{c:"+"}})],1),a("mjx-mi",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"221E"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"]"}})],1)],1)],1),t._v(" 映射到 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"["}})],1),a("mjx-mn",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"0"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:","}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"2"}},[a("mjx-c",{attrs:{c:"+"}})],1),a("mjx-mi",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"221E"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"]"}})],1)],1)],1),t._v("，而在类标这里起到了指示的作用，因此我们一般设置类标为 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mn",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"0"}})],1)],1)],1),t._v(" 和 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mn",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"1"}})],1)],1)],1),t._v("。")],1),t._v(" "),a("p",[t._v("而上面的描述其实就是"),a("strong",[t._v("交叉熵损失函数")]),t._v("，交叉熵损失函数计算损失的特点是：损失只由类标的预测概率决定，如果类标所对应的预测概率越大，损失越小，如果类标所对应的预测概率越小，损失越大。")]),t._v(" "),a("p",[t._v("而交叉熵损失函数等价于极大似然估计，即交叉熵最小化等价于对数似然极大化。这是我看到的很多介绍逻辑回归的资料上面的两种对目标函数的定义，即：")]),t._v(" "),a("p",[t._v("1、目标函数如果是交叉熵：则最小化；\n2、目标函数如果是对数似然，则极大化。")]),t._v(" "),a("p",[t._v("二者是等价的，它们的解析表示式也仅仅只相差一个负号。")]),t._v(" "),a("h3",{attrs:{id:"交叉熵损失函数梯度公式的推导"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#交叉熵损失函数梯度公式的推导"}},[t._v("#")]),t._v(" 交叉熵损失函数梯度公式的推导")]),t._v(" "),a("p",[t._v("二分类问题逻辑回归的损失函数的梯度函数公式的推导我写在这里了："),a("a",{attrs:{href:"https://www.jianshu.com/p/b8249e6ee85f",target:"_blank",rel:"noopener noreferrer"}},[t._v("二分类问题逻辑回归的损失函数的梯度函数"),a("OutboundLink")],1),t._v("。下面是手写笔记：")]),t._v(" "),a("p",[a("img",{attrs:{src:"http://upload-images.jianshu.io/upload_images/414598-c00719b5ab140f49.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240",alt:"交叉熵损失函数梯度公式的推导"}})]),t._v(" "),a("h2",{attrs:{id:"逻辑回归应用于多分类问题"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#逻辑回归应用于多分类问题"}},[t._v("#")]),t._v(" 逻辑回归应用于多分类问题")]),t._v(" "),a("p",[t._v("一般地，二分类问题应用于多分类问题又两种思路：1、一对一；2、一对多。可以看一看 scikit-learn 文档对于“一对一”和“一对多”的描述。")]),t._v(" "),a("p",[t._v("1、一对一\n"),a("a",{attrs:{href:"https://scikit-learn.org/stable/modules/multiclass.html#ovo-classification",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://scikit-learn.org/stable/modules/multiclass.html#ovo-classification"),a("OutboundLink")],1),t._v(" "),a("a",{attrs:{href:"https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsOneClassifier.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsOneClassifier.html"),a("OutboundLink")],1),t._v("\n2、一对多\n"),a("a",{attrs:{href:"https://scikit-learn.org/stable/modules/multiclass.html#ovr-classification",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://scikit-learn.org/stable/modules/multiclass.html#ovr-classification"),a("OutboundLink")],1),t._v(" "),a("a",{attrs:{href:"https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html"),a("OutboundLink")],1)]),t._v(" "),a("p",[t._v("中文："),a("a",{attrs:{href:"http://sklearn.apachecn.org/#/docs/13?id=_112-%E5%A4%9A%E7%B1%BB%E5%92%8C%E5%A4%9A%E6%A0%87%E7%AD%BE%E7%AE%97%E6%B3%95",target:"_blank",rel:"noopener noreferrer"}},[t._v("http://sklearn.apachecn.org/#/docs/13?id=_112-%E5%A4%9A%E7%B1%BB%E5%92%8C%E5%A4%9A%E6%A0%87%E7%AD%BE%E7%AE%97%E6%B3%95"),a("OutboundLink")],1)]),t._v(" "),a("p",[t._v("而逻辑回归应用于多分类问题可以使用更精准的，我认为是特殊的“一对一”的方法：softmax 回归。")]),t._v(" "),a("h3",{attrs:{id:"softmax-回归"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#softmax-回归"}},[t._v("#")]),t._v(" softmax 回归")]),t._v(" "),a("p",[t._v("在很多资料上我们会看到 softmax 回归，其实我觉得 softmax 回归和逻辑回归是一回事。逻辑回归的 sigmoid 函数把取值映射成概率，softmax 函数做的也是相同的事情。")]),t._v(" "),a("p",[t._v("softmax 函数把取值映射成概率的做法是先取指数函数，目的是把取值映射到 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"["}})],1),a("mjx-mn",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"0"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:","}})],1),a("mjx-mo",{staticClass:"mjx-n",attrs:{space:"2"}},[a("mjx-c",{attrs:{c:"+"}})],1),a("mjx-mi",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"221E"}})],1),a("mjx-mo",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"]"}})],1)],1)],1),t._v("，然后再归一化，因为如果不把取值映射成正数，归一化就无从下手，更不能解释概率是负数的问题了（虽然描述可能不太准确，但我想意思还是能表达清楚）。")],1),t._v(" "),a("p",[t._v("在深度学习中，如果是二分类问题，输出层使用的激活函数是 sigmoid，而多分类问题，输出层使用的激活函数是 softmax。")]),t._v(" "),a("h2",{attrs:{id:"scikit-learn-的逻辑回归"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#scikit-learn-的逻辑回归"}},[t._v("#")]),t._v(" scikit-learn 的逻辑回归")]),t._v(" "),a("p",[t._v("1、scikit-learn 的逻辑回归加上了正则化项，正则化的强度可以由使用者来指定。")]),t._v(" "),a("p",[t._v("参数 C 是一个浮点数，默认的值是 "),a("mjx-container",{staticClass:"MathJax",attrs:{jax:"CHTML"}},[a("mjx-math",{staticClass:" MJX-TEX"},[a("mjx-mn",{staticClass:"mjx-n"},[a("mjx-c",{attrs:{c:"1"}}),a("mjx-c",{attrs:{c:"."}}),a("mjx-c",{attrs:{c:"0"}})],1)],1)],1),t._v("。这个 C 是正则化强度的倒数，如果 C 越小，正则化强度越大。")],1),t._v(" "),a("p",[t._v("2、对于多分类问题，不同的多分类的方法对应了可以使用的不同的训练方法，这一点需要参考文档。")]),t._v(" "),a("h3",{attrs:{id:"最大熵模型"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#最大熵模型"}},[t._v("#")]),t._v(" 最大熵模型")]),t._v(" "),a("p",[t._v("逻辑回归其实是最大熵模型的特例，我另外写了学习笔记。")]),t._v(" "),a("p",[t._v("（本文完）")]),t._v(" "),a("h2",{attrs:{id:"参考资料"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#参考资料"}},[t._v("#")]),t._v(" 参考资料")]),t._v(" "),a("p",[t._v("[1] 李航. 统计学习方法（第 2 版）第 6 章“逻辑回归”. 北京：清华大学出版社，2019.")]),t._v(" "),a("p",[t._v("[2] 周志华. 机器学习（第 3 章第 3 节“对数几率回归”）. 北京：清华大学出版社.")]),t._v(" "),a("hr"),t._v(" "),a("p",[t._v("以下为草稿，我自己留着备用，读者可以忽略，欢迎大家批评指正。")]),t._v(" "),a("h3",{attrs:{id:"参考资料-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#参考资料-2"}},[t._v("#")]),t._v(" 参考资料")]),t._v(" "),a("p",[t._v("1、scikit-learn 官方文档：\n英文："),a("a",{attrs:{href:"https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression"),a("OutboundLink")],1),t._v("\n中文："),a("a",{attrs:{href:"http://sklearn.apachecn.org/#/docs/2?id=_1111-logistic-%E5%9B%9E%E5%BD%92",target:"_blank",rel:"noopener noreferrer"}},[t._v("http://sklearn.apachecn.org/#/docs/2?id=_1111-logistic-回归"),a("OutboundLink")],1)]),t._v(" "),a("p",[t._v("2、刘建平的文章：\n（1）逻辑回归原理小结\n地址：https://www.cnblogs.com/pinard/p/6029432.html\n（2）scikit-learn 逻辑回归类库使用小结\n地址：https://www.cnblogs.com/pinard/p/6035872.html")]),t._v(" "),a("p",[t._v("3、Aurélien，Géron 著：《Sklearn 与 TensorFlow 机器学习实用指南》")]),t._v(" "),a("p",[t._v("官方 GitHub 代码仓库："),a("a",{attrs:{href:"https://github.com/ageron/handson-ml/blob/master/04_training_linear_models.ipynb",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://github.com/ageron/handson-ml/blob/master/04_training_linear_models.ipynb"),a("OutboundLink")],1)]),t._v(" "),a("p",[t._v("在 nbviewer 上查看："),a("a",{attrs:{href:"https://nbviewer.jupyter.org/github/ageron/handson-ml/blob/master/04_training_linear_models.ipynb",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://nbviewer.jupyter.org/github/ageron/handson-ml/blob/master/04_training_linear_models.ipynb"),a("OutboundLink")],1)]),t._v(" "),a("p",[t._v("4、开源组织 ApacheCN 也对这本书做了翻译：ApacheCN\n地址："),a("a",{attrs:{href:"https://www.bookstack.cn/read/hands_on_Ml_with_Sklearn_and_TF/spilt.6.docs-4.%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B.md",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://www.bookstack.cn/read/hands_on_Ml_with_Sklearn_and_TF/spilt.6.docs-4.%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B.md"),a("OutboundLink")],1)]),t._v(" "),a("p",[t._v("5、GLM(广义线性模型) 与 LR(逻辑回归) 详解")]),t._v(" "),a("p",[t._v("https://blog.csdn.net/Cdd2xd/article/details/75635688")])])}),[],!1,null,null,null);s.default=c.exports}}]);